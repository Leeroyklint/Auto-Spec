"""backend/main.py ‚Äì v2
-------------------------------------------------------------
‚Ä¢ Ajoute des consignes pour obtenir des r√©ponses p√©dagogiques
  (explication en fran√ßais, pas de formule DAX brute sauf demande).
‚Ä¢ L√©g√®re refacto + constante MAX_JSON pour ajuster la taille du
  contexte transmis au LLM.
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from pydantic import BaseModel
import tempfile
import json
from typing import Dict

from .extract_pbix import extract_spec
from .generate_narrative import generate_narrative
from common.azure_llm import azure_llm_chat

app = FastAPI(title="Klint PBIX Spec & Chat API", version="2.0")

# ----------------------------------------------------------------------------------------------------------------------
# Cache m√©moire : id_spec -> JSON technique (‚ö†Ô∏è non persistant ‚Üí pr√©voir Redis pour le multi-instance)
# ----------------------------------------------------------------------------------------------------------------------
CACHE: Dict[str, Dict] = {}
MAX_JSON_LENGTH = 8_000   # caract√®res de contexte pour le LLM

# ------------------------------------------------------------
# Pydantic Schemas
# ------------------------------------------------------------
class SpecResponse(BaseModel):
    id: str
    functional: str   # markdown
    technical: dict   # json brut


class ChatRequest(BaseModel):
    id: str
    question: str


class ChatResponse(BaseModel):
    answer: str


# ------------------------------------------------------------
# Endpoint SPEC : /api/spec
# ------------------------------------------------------------
@app.post("/api/spec", response_model=SpecResponse)
async def build_spec(pbix: UploadFile = File(...)):
    """Extraction des m√©tadonn√©es d‚Äôun .pbix et g√©n√©ration de la sp√©cification fonctionnelle."""
    # --- 1) Sauvegarde temporaire du fichier ---------------------------------------------------
    try:
        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".pbix")
        tmp_file.write(await pbix.read())
        tmp_file.close()
    except Exception as exc:
        raise HTTPException(500, f"Erreur lors de la sauvegarde du PBIX : {exc}")

    # --- 2) Extraction technique + r√©daction fonctionnelle --------------------------------------
    technical = extract_spec(tmp_file.name)
    functional = generate_narrative(technical)

    # --- 3) Cache & r√©ponse --------------------------------------------------------------------
    CACHE[technical["id"]] = technical
    return {
        "id": technical["id"],
        "functional": functional,
        "technical": technical,
    }


# ------------------------------------------------------------
# Endpoint CHAT : /api/chat
# ------------------------------------------------------------
@app.post("/api/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    tech = CACHE.get(req.id)
    if tech is None:
        raise HTTPException(400, "‚õî PBIX non charg√©. Recharge d‚Äôabord un fichier.")

    question = req.question.strip()
    if not question:
        return {"answer": "Pose une vraie question üòâ"}

    # --------------------------- PROMPT LLM ----------------------------------------------------
    system_content = (
        "Tu es un expert Power BI. Tu dois r√©pondre *uniquement* en te basant sur le mod√®le ¬´ GreenTech √©quipe 1 ¬ª "
        "fourni ci-dessous (tronqu√© pour rester dans la limite) :\n\n" + json.dumps(tech)[:MAX_JSON_LENGTH] +
        "\n\nüëâ Si l‚Äôutilisateur demande *comment* une mesure est calcul√©e, explique-le en fran√ßais clair et p√©dagogique, "
        "sans afficher la formule DAX compl√®te tant qu‚Äôil ne la r√©clame pas explicitement. Utilise des exemples concrets "
        "au besoin (pourcentage, somme, ratio, etc.)."
    )

    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": question},
    ]

    answer, _ = azure_llm_chat(messages)
    return {"answer": answer}
